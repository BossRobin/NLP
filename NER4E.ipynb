{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15wtEKSvuSaA9Ffb1VuXjltL1-d683TtM",
      "authorship_tag": "ABX9TyMLasvfeO3w+iA/MX5k19rO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BossRobin/NLP/blob/master/NER4E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxQGK_xcsAut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "51984485-589f-4c5d-9d82-940d893c45a8"
      },
      "source": [
        "#! coding=utf-8\n",
        "# FIRST=======================统计的方法\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# latin1是Mysql的默认字符集\n",
        "data = pd.read_csv(\"drive/My Drive/ner_dataset.csv\", encoding=\"latin1\")\n",
        "# ffill是向下填充，即NaN值填充为它上面的值，bfill是向上填充。\n",
        "data = data.fillna(method=\"ffill\")\n",
        "print(data.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Sentence #           Word  POS    Tag\n",
            "0  Sentence: 1      Thousands  NNS      O\n",
            "1  Sentence: 1             of   IN      O\n",
            "2  Sentence: 1  demonstrators  NNS      O\n",
            "3  Sentence: 1           have  VBP      O\n",
            "4  Sentence: 1        marched  VBN      O\n",
            "5  Sentence: 1        through   IN      O\n",
            "6  Sentence: 1         London  NNP  B-geo\n",
            "7  Sentence: 1             to   TO      O\n",
            "8  Sentence: 1        protest   VB      O\n",
            "9  Sentence: 1            the   DT      O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsQ4GjAGtK_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2dcf8a20-2faf-4334-97ed-e79e0fec0719"
      },
      "source": [
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "\n",
        "print(n_words, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35178 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nghW_7F-tT5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b14f61e0-c448-470b-a7db-1f4c9f85d1f7"
      },
      "source": [
        "# 预处理数据，生成训练集和标签\n",
        "class SentenceGetter(object):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.n_sent = 1\n",
        "    self.empty = False\n",
        "  \n",
        "  def get_next(self):\n",
        "    try:\n",
        "      s = self.data[self.data[\"Sentence #\"] == \"Sentence: {}\".format(self.n_sent)]\n",
        "      self.n_sent += 1\n",
        "      return s[\"Word\"].values.tolist(), s[\"POS\"].values.tolist(), s[\"Tag\"].values.tolist()\n",
        "    except:\n",
        "      self.empty = True\n",
        "      return None, None, None\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sent, pos, tag = getter.get_next()\n",
        "print(sent, pos, tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'] ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.'] ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLpqi67XuE0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c3ea3c54-3ff1-42ba-d5b1-f2189d0c70fa"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class MemoryTagger(BaseEstimator, TransformerMixin):\n",
        "  # fit即train\n",
        "  def fit(self, X, y):\n",
        "    voc = {}\n",
        "    self.tags = []\n",
        "    for w, t in zip(X, y):\n",
        "      # self.tags统计标签的种类\n",
        "      if t not in self.tags:\n",
        "        self.tags.append(t)\n",
        "      # voc统计训练集中的每一个词对应的标签出现的次数\n",
        "      if w not in voc:\n",
        "        voc[w] = {}\n",
        "      if t not in voc[w]:\n",
        "        voc[w][t] = 0\n",
        "      voc[w][t] += 1\n",
        "    # self.memory统计的是字典中每一个词对应的出现频次最大的标签\n",
        "    self.memory = {}\n",
        "    for k, d in voc.items():\n",
        "      self.memory[k] = max(d, key=d.get)\n",
        "  def predict(self, X, y=None):\n",
        "    # 返回词汇x对应的标签，如果memory中没有，返回0.\n",
        "    return [self.memory.get(x, \"O\") for x in X]\n",
        "# 测试一句\n",
        "tagger = MemoryTagger()\n",
        "tagger.fit(sent, tag)\n",
        "print(tagger.predict(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6nnRxXQu3F2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "8c6ef79b-12da-44f0-b231-bda2604c4de1"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import classification_report\n",
        "# 得到所有的词汇\n",
        "words = data[\"Word\"].values.tolist()\n",
        "# 得到所有词汇对应的标签\n",
        "tags = data[\"Tag\"].values.tolist()\n",
        "# 因为MemoryTagger继承了(BaseEstimator, TransformerMixin)，所以可以直接使用sklearn的cross_val_predict进行交叉验证。\n",
        "pred = cross_val_predict(estimator=MemoryTagger(), X=words, y=tags, cv=5)\n",
        "report = classification_report(y_pred=pred, y_true=tags)\n",
        "print(report, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.20      0.05      0.09       402\n",
            "       B-eve       0.54      0.25      0.34       308\n",
            "       B-geo       0.78      0.85      0.81     37644\n",
            "       B-gpe       0.94      0.93      0.94     15870\n",
            "       B-nat       0.42      0.28      0.33       201\n",
            "       B-org       0.67      0.49      0.56     20143\n",
            "       B-per       0.78      0.65      0.71     16990\n",
            "       B-tim       0.87      0.77      0.82     20333\n",
            "       I-art       0.04      0.01      0.01       297\n",
            "       I-eve       0.39      0.12      0.18       253\n",
            "       I-geo       0.73      0.58      0.65      7414\n",
            "       I-gpe       0.62      0.45      0.52       198\n",
            "       I-nat       0.00      0.00      0.00        51\n",
            "       I-org       0.69      0.53      0.60     16784\n",
            "       I-per       0.73      0.65      0.69     17251\n",
            "       I-tim       0.58      0.13      0.21      6528\n",
            "           O       0.97      0.99      0.98    887908\n",
            "\n",
            "    accuracy                           0.95   1048575\n",
            "   macro avg       0.59      0.45      0.50   1048575\n",
            "weighted avg       0.94      0.95      0.94   1048575\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YckKhov1zqsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "deab70b0-50b9-43a7-f606-8bb13581e20a"
      },
      "source": [
        "# SECOND=======================机器学习方法\n",
        "# 使用随机森林分类器\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# 简单的构造特征的函数\n",
        "def feature_map(word):\n",
        "  return np.array([word.istitle(), word.islower(), word.isupper(), len(word), word.isdigit(), word.isalpha()])\n",
        "# 得到每个词的特征\n",
        "words = [feature_map(w) for w in data[\"Word\"].values.tolist()]\n",
        "# 开始训练\n",
        "pred = cross_val_predict(RandomForestClassifier(n_estimators=20), X=words, y=tags, cv=5)\n",
        "report = classification_report(y_pred=pred, y_true=tags)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.00      0.00      0.00       402\n",
            "       B-eve       0.00      0.00      0.00       308\n",
            "       B-geo       0.26      0.79      0.40     37644\n",
            "       B-gpe       0.26      0.06      0.09     15870\n",
            "       B-nat       0.00      0.00      0.00       201\n",
            "       B-org       0.65      0.17      0.27     20143\n",
            "       B-per       0.97      0.20      0.33     16990\n",
            "       B-tim       0.29      0.32      0.30     20333\n",
            "       I-art       0.00      0.00      0.00       297\n",
            "       I-eve       0.00      0.00      0.00       253\n",
            "       I-geo       0.00      0.00      0.00      7414\n",
            "       I-gpe       0.00      0.00      0.00       198\n",
            "       I-nat       0.00      0.00      0.00        51\n",
            "       I-org       0.36      0.03      0.06     16784\n",
            "       I-per       0.47      0.02      0.04     17251\n",
            "       I-tim       0.50      0.06      0.11      6528\n",
            "           O       0.97      0.98      0.97    887908\n",
            "\n",
            "    accuracy                           0.87   1048575\n",
            "   macro avg       0.28      0.15      0.15   1048575\n",
            "weighted avg       0.88      0.87      0.86   1048575\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2tdhVsf4GjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# THIRD====================CRF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv(\"drive/My Drive/ner_dataset.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "\n",
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfz3lS5s5dPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "eed8f5e5-15fb-4fa0-fbe5-715197be8170"
      },
      "source": [
        "\"\"\"我们的数据中共有47959个句子，其中包含了35178个单词。 \n",
        "现在来构建一个输出句子的构造器。\n",
        "和第一种方法类似\n",
        "\"\"\"\n",
        "class SentenceGetter(object):\n",
        "  def __init__(self, data):\n",
        "    self.n_sent = 0\n",
        "    self.data = data\n",
        "    self.empty = False\n",
        "    agg_func = lambda s : [(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),s[\"POS\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
        "    self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "    self.sentences = [s for s in self.grouped]\n",
        "  def get_next(self):\n",
        "    try:\n",
        "      s = self.sentences[self.n_sent]\n",
        "      self.n_sent += 1\n",
        "      return s\n",
        "    except:\n",
        "      self.empty = True\n",
        "      return None\n",
        "getter = SentenceGetter(data)\n",
        "sent = getter.get_next()\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMN0rx6q_bAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 每个句子都是词的三元组构成\n",
        "sentences = getter.sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D-2Mf6f__FT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "c67c57af-04b5-40f4-c47e-e5fbb30de66f"
      },
      "source": [
        "# 将每个词转化为特征向量\n",
        "def word2features(sent, i):\n",
        "  # sent[i][0]是第i个单词的token值，sent[i][1]是第i个单词的pos值\n",
        "  word = sent[i][0]\n",
        "  postag = sent[i][1]\n",
        "  # 针对单个单词的特征函数\n",
        "  features = {\n",
        "      \"bias\": 1.0,\n",
        "      \"word.lower()\": word.lower(),\n",
        "      \"word[-3:]\": word[-3:],\n",
        "      \"word[2:]\": word[2:],\n",
        "      \"word.isupper()\": word.isupper(),\n",
        "      \"word.istitle()\": word.istitle(),\n",
        "      \"word.isdigit()\": word.isdigit(),\n",
        "      \"postag\": postag,\n",
        "      \"postag[:2]\": postag[:2]\n",
        "  }\n",
        "  # 针对有前缀单词的单词的特征函数\n",
        "  if i > 0:\n",
        "    word1 = sent[i-1][0]\n",
        "    postag1 = sent[i-1][1]\n",
        "    features.update({\n",
        "        \"-1:word.isupper()\": word1.isupper(),\n",
        "        \"-1:word.istitle()\": word1.istitle(),\n",
        "        \"-1:word.isdigit()\": word1.isdigit(),\n",
        "        \"-1:postag\": postag1,\n",
        "        \"-1:postag[:2]\": postag1[:2]\n",
        "    })\n",
        "  else:\n",
        "    features[\"BOS\"] = True\n",
        "  # 针对有后缀单词的单词的特征函数\n",
        "  if i < len(sent) - 1:\n",
        "    word1 = sent[i+1][0]\n",
        "    postag1 = sent[i+1][1]\n",
        "    features.update({\n",
        "      \"+1:word.isupper()\": word1.isupper(),\n",
        "      \"+1:word.istitle()\": word1.istitle(),\n",
        "      \"+1:word.isdigit()\": word1.isdigit(),\n",
        "      \"+1:postag\": postag1,\n",
        "      \"+1:postag[:2]\": postag1[:2]\n",
        "    })\n",
        "  else:\n",
        "    features[\"EOS\"] = True\n",
        "  return features\n",
        "# 将一个句子转化为特征向量\n",
        "def sent2features(sent):\n",
        "  return [word2features(sent, i) for i in range(len(sent))]\n",
        "# 提取句子中的每个单词对应的标签\n",
        "def sent2labels(sent):\n",
        "  return [label for _, _, label in sent]\n",
        "# 提取句子中单词的token\n",
        "def sent2tokens(sent):\n",
        "  return [token for token, _, _ in sent]\n",
        "\n",
        "print(sent2features(sentences[0]))\n",
        "\n",
        "print(sent2labels(sentences[0]))\n",
        "\n",
        "print(sent2tokens(sentences[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'bias': 1.0, 'word.lower()': 'thousands', 'word[-3:]': 'nds', 'word[2:]': 'ousands', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NNS', 'postag[:2]': 'NN', 'BOS': True, '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'of', 'word[-3:]': 'of', 'word[2:]': '', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.isupper()': False, '-1:word.istitle()': True, '-1:word.isdigit()': False, '-1:postag': 'NNS', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'NNS', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'demonstrators', 'word[-3:]': 'ors', 'word[2:]': 'monstrators', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NNS', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'VBP', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'have', 'word[-3:]': 'ave', 'word[2:]': 've', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'VBP', 'postag[:2]': 'VB', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'NNS', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'VBN', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'marched', 'word[-3:]': 'hed', 'word[2:]': 'rched', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'VBN', 'postag[:2]': 'VB', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'VBP', '-1:postag[:2]': 'VB', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'through', 'word[-3:]': 'ugh', 'word[2:]': 'rough', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'VBN', '-1:postag[:2]': 'VB', '+1:word.isupper()': False, '+1:word.istitle()': True, '+1:word.isdigit()': False, '+1:postag': 'NNP', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'london', 'word[-3:]': 'don', 'word[2:]': 'ndon', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NNP', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'TO', '+1:postag[:2]': 'TO'}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[2:]': '', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'TO', 'postag[:2]': 'TO', '-1:word.isupper()': False, '-1:word.istitle()': True, '-1:word.isdigit()': False, '-1:postag': 'NNP', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'VB', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'protest', 'word[-3:]': 'est', 'word[2:]': 'otest', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'VB', 'postag[:2]': 'VB', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'TO', '-1:postag[:2]': 'TO', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[2:]': 'e', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'VB', '-1:postag[:2]': 'VB', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'war', 'word[-3:]': 'war', 'word[2:]': 'r', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'in', 'word[-3:]': 'in', 'word[2:]': '', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': True, '+1:word.isdigit()': False, '+1:postag': 'NNP', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'iraq', 'word[-3:]': 'raq', 'word[2:]': 'aq', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NNP', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'CC', '+1:postag[:2]': 'CC'}, {'bias': 1.0, 'word.lower()': 'and', 'word[-3:]': 'and', 'word[2:]': 'd', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'CC', 'postag[:2]': 'CC', '-1:word.isupper()': False, '-1:word.istitle()': True, '-1:word.isdigit()': False, '-1:postag': 'NNP', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'VB', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'demand', 'word[-3:]': 'and', 'word[2:]': 'mand', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'VB', 'postag[:2]': 'VB', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'CC', '-1:postag[:2]': 'CC', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[2:]': 'e', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'VB', '-1:postag[:2]': 'VB', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'withdrawal', 'word[-3:]': 'wal', 'word[2:]': 'thdrawal', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'of', 'word[-3:]': 'of', 'word[2:]': '', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': True, '+1:word.isdigit()': False, '+1:postag': 'JJ', '+1:postag[:2]': 'JJ'}, {'bias': 1.0, 'word.lower()': 'british', 'word[-3:]': 'ish', 'word[2:]': 'itish', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'JJ', 'postag[:2]': 'JJ', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'NNS', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'troops', 'word[-3:]': 'ops', 'word[2:]': 'oops', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NNS', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': True, '-1:word.isdigit()': False, '-1:postag': 'JJ', '-1:postag[:2]': 'JJ', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'from', 'word[-3:]': 'rom', 'word[2:]': 'om', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'NNS', '-1:postag[:2]': 'NN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'that', 'word[-3:]': 'hat', 'word[2:]': 'at', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'country', 'word[-3:]': 'try', 'word[2:]': 'untry', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.isupper()': False, '+1:word.istitle()': False, '+1:word.isdigit()': False, '+1:postag': '.', '+1:postag[:2]': '.'}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[2:]': '', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': '.', 'postag[:2]': '.', '-1:word.isupper()': False, '-1:word.istitle()': False, '-1:word.isdigit()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', 'EOS': True}]\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
            "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTp-8GITBPmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 得到所有句子的特征向量和标签\n",
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VLwjKeKBY8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "b207ad9e-dda1-479d-c5b1-a83a6be1b424"
      },
      "source": [
        "from sklearn_crfsuite import CRF\n",
        "# 构建CRF分类器\n",
        "crf = CRF(\n",
        "    algorithm = \"lbfgs\",# lbfgs为无约束优化算法\n",
        "    c1 = 0.1, # L1正则化的力度\n",
        "    c2 = 0.1, # L2正则化的力度\n",
        "    max_iterations = 100,\n",
        "    all_possible_transitions = False\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "pred = cross_val_predict(estimator=crf,X=X,y=y,cv=5)\n",
        "\n",
        "report = flat_classification_report(y_pred=pred, y_true=y)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.33      0.12      0.17       402\n",
            "       B-eve       0.53      0.37      0.43       308\n",
            "       B-geo       0.84      0.91      0.87     37644\n",
            "       B-gpe       0.97      0.94      0.96     15870\n",
            "       B-nat       0.74      0.37      0.49       201\n",
            "       B-org       0.79      0.70      0.74     20143\n",
            "       B-per       0.84      0.80      0.82     16990\n",
            "       B-tim       0.93      0.81      0.87     20333\n",
            "       I-art       0.14      0.04      0.07       297\n",
            "       I-eve       0.36      0.24      0.29       253\n",
            "       I-geo       0.80      0.78      0.79      7414\n",
            "       I-gpe       0.92      0.54      0.68       198\n",
            "       I-nat       0.62      0.29      0.40        51\n",
            "       I-org       0.80      0.79      0.79     16784\n",
            "       I-per       0.83      0.90      0.86     17251\n",
            "       I-tim       0.82      0.73      0.77      6528\n",
            "           O       0.99      0.99      0.99    887908\n",
            "\n",
            "    accuracy                           0.97   1048575\n",
            "   macro avg       0.72      0.61      0.65   1048575\n",
            "weighted avg       0.97      0.97      0.97   1048575\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnEAb2dnCnhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install eli5\n",
        "\"\"\"可视化tags间的转移分数以及各features的重要性\"\"\"\n",
        "crf.fit(X, y)\n",
        "import eli5\n",
        "eli5.show_weights(crf, top=30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT_CXXgKKHUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=10, # 通过增加c1增加L1正则化的力度，使features变稀疏\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=False\n",
        ")\n",
        "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
        "report = classification_report(y_pred=pred, y_true=y)\n",
        "print(report)\n",
        "\n",
        "crf.fit(X, y)\n",
        "\n",
        "\n",
        "eli5.show_weights(crf, top=30)\n",
        "\n",
        "\"\"\"增加L1正则后，features减少，但是模型效果仍旧不错\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdzKxFIByeSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# FORTH================BERT\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv(\"drive/My Drive/ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peta-Kidz2Yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "18329fab-3680-4bc4-cb57-4d626aae4217"
      },
      "source": [
        "\"\"\"构建SentenceGetter\"\"\"\n",
        "class SentenceGetter(object):\n",
        "  def __init__(self, data):\n",
        "    self.n_sent = 1\n",
        "    self.data = data\n",
        "    self.empty = False\n",
        "    agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),s[\"POS\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
        "    self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "    self.sentences = [s for s in self.grouped]\n",
        "  \n",
        "  def get_next(self):\n",
        "    try:\n",
        "      s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "      self.n_sent += 1\n",
        "      return s\n",
        "    except:\n",
        "      self.empty = True\n",
        "      return None\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "\n",
        "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
        "print(sentences[0])\n",
        "\n",
        "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
        "print(labels[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAJUIN81Isv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"构建tag词典\"\"\"\n",
        "tags_vals = list(set(data[\"Tag\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrU7ZDe1WUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# !pip install pytorch_pretrained_bert\n",
        "\"\"\"导入相关库\"\"\"\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LymIjjpVBtQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd5395cb-d8a9-4b6b-fc07-7971ec81d723"
      },
      "source": [
        "\"\"\"设置基本参数\"\"\"\n",
        "max_len = 60\n",
        "batch_size = 32\n",
        "\n",
        "\"\"\"设置device\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Klk6w3C3_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "3893a67c-506c-4689-dc14-3804d0c39299"
      },
      "source": [
        "\"\"\"tokenize处理\"\"\"\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print(tokenized_texts[0])\n",
        "\n",
        "\"\"\"将输入转化为id 并且 截长补短\"\"\"\n",
        "# 截断补全操作。对于大于max_len的句子进行截断，不足的则补全。truncating=\"post\"意思是后面截断，padding=\"post\"意思是后面补全。\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(input_ids[0])\n",
        "\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=max_len, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "print(tags[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\n",
            "[ 5190  1997 28337  2031  9847  2083  2414  2000  6186  1996  2162  1999\n",
            "  5712  1998  5157  1996 10534  1997  2329  3629  2013  2008  2406  1012\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
            "[7 7 7 7 7 7 8 7 7 7 7 7 8 7 7 7 7 7 0 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
            " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WPGeUx8DXVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "75868b9b-40c9-43a2-dc03-57f3169bc6d0"
      },
      "source": [
        "\"\"\"准备mask_attention\"\"\"\n",
        "# padding的位置不用mask，设置为0，其余为1\n",
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4t9py8wEZu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"将数据进行划分\"\"\"\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=2019, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2019, test_size=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh3WQBEiEjI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"将数据转化为tensor的形式\"\"\"\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJzyxTAEmkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"定义dataloader,在训练阶段shuffle数据，预测阶段不需要shuffle\"\"\"\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data) #预测阶段需要shuffle\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)  #测试阶段不需要shuffle\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wDk2luEyeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"**开始训练过程**\"\"\"\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLPfDGI9FHxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6cba512-ece1-47f5-99a3-455c6f0fe706"
      },
      "source": [
        "\"\"\"定义optimizer(分为是否调整全部参数两种情况)\"\"\"\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    # 列举模型的全部参数名\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta'] # 不需要正则化的参数\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWPgkbFFR4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "20999a6b-2c93-4801-9123-8546d2ab91a4"
      },
      "source": [
        "# !pip install seqeval"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=128cc05feb2c3b713c36c10546c76a5653d856fa7f73dc8afb180e08f600eeab\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Tcp92VFUyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"定义评估accuracy的函数\n",
        "\n",
        "f1: https://blog.csdn.net/qq_37466121/article/details/87719044\n",
        "\"\"\"\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g12p-51F5FK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCMZOHN1FZ9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "ca21fae4-80f7-41d4-d42e-36859a3d1d07"
      },
      "source": [
        "\"\"\"开始微调过程，建议4个左右epochs\"\"\"\n",
        "epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "for _ in range(epochs): # trange有可视化功能\n",
        "  # 训练过程\n",
        "  model.train()\n",
        "  tr_loss = 0\n",
        "  nb_tr_steps = 0\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # 将batch设置为gpu模式\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # 前向过程\n",
        "    loss = model(b_input_ids, token_type_ids=None,\n",
        "                  attention_mask=b_input_mask, labels=b_labels)\n",
        "    # 后向过程\n",
        "    loss.backward()\n",
        "    # 损失\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_steps += 1\n",
        "    # 梯度裁剪\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    # 更新参数\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "  #打印每个epoch的损失\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "  # 验证过程\n",
        "  model.eval()\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps = 0\n",
        "  predictions , true_labels = [], []\n",
        "  for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # no_grad是不需要反向传播梯度\n",
        "    with torch.no_grad():\n",
        "      # 因为误差不反向传播，所以可以使用验证数据集当成训练集输入模型得到验证集上的误差。\n",
        "      tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "      # 得到真正的模型预测的验证集的结果，以softmax形式输出\n",
        "      logits = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "    logits = logits.detach().cpu().numpy()#detach的方法，将variable参数从网络中隔离开，不参与参数更新\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # print(\"label_ids\", label_ids)\n",
        "    # print(\"np.argmax(logits, axis=2)\", np.argmax(logits, axis=2))\n",
        "    # predictions保存了每个句子中每个词预测的tag结果\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.append(label_ids)\n",
        "    # 计算accuracy 和 loss\n",
        "    # 当前batch的验证集的准确率\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  # 打印信息\n",
        "  print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "  valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "  print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))#传入的是具体的tag\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.15560755134457302\n",
            "Validation loss: 0.1590871246655782\n",
            "Validation Accuracy: 0.9327579365079361\n",
            "F1-Score: 0.48563854670201156\n",
            "Train loss: 0.11980487075423735\n",
            "Validation loss: 0.14978278296689193\n",
            "Validation Accuracy: 0.9338953373015872\n",
            "F1-Score: 0.5020317130549021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-573097c807d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 前向过程\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss = model(b_input_ids, token_type_ids=None,\n\u001b[0;32m---> 15\u001b[0;31m                   attention_mask=b_input_mask, labels=b_labels)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 后向过程\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0mactive_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                 \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}